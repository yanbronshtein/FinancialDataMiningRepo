---
title: "HW2_DataMining"
author: "Yaniv Bronshtein"
date: "3/9/2021"
output: html_document
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**import libraries**
```{r}
library(tidyverse)
library(glmnet)
library(Matrix)
library(rsample)
library(ISLR)
library(pls)
```
## Question 3
*Bootstrap with Least squares, Ridge and Lasso*
Let $\beta = (\beta_1,\beta_2, ..., \beta_p)$ and let x,y be random variables such that the entries of x are i.i.d standard normal variables (i.e, with mean zero and variance one) and $y = \beta^Tx + \epsilon$ where $\epsilon \sim N(0,1)$.
(a). Simulate a dataset $(x_1, y_1), ...(x_n, y_n)$ as n i.i.d copies of the random variables x,y defined above, with $n=800$, $p = 200$, and $\beta_j = j^{-1}$.
```{r}
#Define n and p
p <- 200
n <- 800
#Construct x by create p copies of rnorm() which will generate n numbers. Each column represents a random variable x_i
x <- replicate(p, rnorm(n = n, mean = 0, sd = 1))
#Create the j vector
j = 1:p
#Beta is j inverted
beta <- j^-1
#Transform beta
beta_t <- beta %>% as.matrix()
#epsilon is a column vector nX1
epsilon <- rnorm(n = n, mean = 0, sd = 1) %>% as.matrix()
#Construct matrix y according to the formula in the question
y = x %*% beta_t + epsilon

#Create a data frame from y and x
df <- data.frame("y"= y, x) 

```
(b) The goal of this problem is to construct confidence intervals for $\beta_1$ using Bootstrap method.

**Specify a 95% confidence interval.**
Use $\psi$ to avoid confusion with alpha in glmnet()
```{r}
psi = 0.05
```

**Generate the bootstraps to be used for all the models**
```{r}
set.seed(1)
samples <-df %>% bootstraps(1000)
```
  (i). Construct confidence intervals for $\beta_1$ by boostrapping the data and applying Least Squares to the boostrapped data set.
**Create a function to return the coefficients of the linear model**
```{r}
get_beta1_lm <- function(data){
  x_mat <- model.matrix(y~.,data)[,-1]
  y <- as.matrix(data['y'])
  lm_model <- lm(y ~ x_mat)
  beta_1_estim <- coef(lm_model)[2] 
  return(beta_1_estim)
}
```


**Create a vector of 1000 bootstrap estimates of beta 1 for least square**
```{r}
estim_lm <- samples$splits %>% 
  map(.,~as.data.frame(.)) %>% 
  map(.,~get_beta1_lm(.)) %>%
  simplify()

estim_lm
```
**Display the confidence interval for the bootstrap estimates by applying the quantile() function to get the lower and upper bounds and median() to get the estimate** 
```{r}
lm_confint <- estim_lm %>% 
  as.data.frame() 
colnames(lm_confint)[1] = "estimate"  
lm_confint <- lm_confint %>%
  summarise(conf.low = quantile(estimate, psi / 2),
            median = median(estimate),
            conf.high = quantile(estimate, 1 - psi / 2))                                       
lm_confint
```
  ii. Construct confidence intervals for $\beta_1$ by boostrapping the data and applying Ridge to the boostrapped data set.
**Define x_mat_cv and y_cv to be used for cv.glmnet() for both ridge and lasso **
```{r}
x_mat_cv = model.matrix(y~.,df)[,-1]
y_cv = as.matrix(df['y'])
```

**Use cv.glmnet() to perform cross validation for ridge to extract the optimal(minimal) lambda to be used in glmnet()**
```{r}
best_lambda_ridge <- (cv.glmnet(x_mat_cv, y_cv, alpha = 0))$lambda.min
```

**Create a function to return the coefficients of the Ridge model**
```{r}
get_beta1_ridge = function(data){
  x_mat = model.matrix(y~.,data)[,-1]
  y = as.matrix(data['y'])
  ridge_model = glmnet(x_mat, y, alpha = 0, lambda = best_lambda_ridge)
  coef_ridge <- coef(ridge_model)
  beta_1_estim <- coef_ridge[2, 1]
  return(beta_1_estim) 
}
```

**Create a vector of 1000 bootstrap estimates of beta 1 for ridge**
```{r}
estim_ridge <- samples$splits %>% 
  map(.,~as.data.frame(.)) %>% 
  map(.,~get_beta1_ridge(.)) %>%
  simplify()

estim_ridge
```

**Display the confidence interval for the bootstrap estimates by applying the quantile() function to get the lower and upper bounds and median() to get the estimate** 
```{r}
ridge_confint <- estim_ridge %>% 
  as.data.frame() 
colnames(ridge_confint)[1] = "estimate"  
ridge_confint <- ridge_confint %>%
  summarise(conf.low = quantile(estimate, psi / 2),
            median = median(estimate),
            conf.high = quantile(estimate, 1 - psi / 2))                                       

ridge_confint
```

  (iii). Construct confidence intervals for $\beta_1$ by boostrapping the data and applying Lasso to the bootstrapped data set.

**Use cv.glmnet() to perform cross validation for lasso to extract the optimal(minimal) lambda to be used in glmnet()**
```{r}
best_lambda_lasso <- (cv.glmnet(x_mat_cv, y_cv, alpha = 1))$lambda.min
```

**Create a function to return the coefficients of the Lasso model**
```{r}
get_beta1_lasso = function(data){
  x_mat = model.matrix(y~.,data)[,-1]
  y = as.matrix(data['y'])
  lasso_model = glmnet(x_mat, y, alpha = 1, lambda = best_lambda_lasso)
  coef_lasso <- coef(lasso_model)
  beta_1_estim <- coef_lasso[2, 1]
  return(beta_1_estim) 
}
```

**Create a vector of 1000 bootstrap estimates of beta 1 for lasso**
```{r}
estim_lasso <- samples$splits %>% 
  map(.,~as.data.frame(.)) %>% 
  map(.,~get_beta1_lasso(.)) %>%
  simplify()

estim_lasso
```

**Display the confidence interval for the bootstrap estimates by applying the quantile() function to get the lower and upper bounds and median() to get the estimate** 
```{r}
lasso_confint <- estim_lasso %>% 
  as.data.frame() 
colnames(lasso_confint)[1] = "estimate"  
lasso_confint <- lasso_confint %>%
  summarise(conf.low = quantile(estimate, psi / 2),
            median = median(estimate),
            conf.high = quantile(estimate, 1 - psi / 2))                                       
lasso_confint
```

(c). Comment on the obtained results
**From the results, we assertain that lasso and least squares produce similar estimates for $\beta_1$ with lasso providing the tightest bound of all 3. Ridge produces a significantly lower estimate than both lasso and least squares with a bound that is not as tight as lasso but significantly tighter than least squares**


## Problem 4(Question 9 in ISL Pg. 263 )
9.In this exercise, we will predict the number of applications received  
using the other variables in the College data set.
```{r}
data(College)
```
(a). Split the data set into a training set and a test set.
set.seed(1)
```{r}
data(College)
set.seed(1)
smp_siz <- dim(College)[1] / 2

train <- sample(seq_len(nrow(College)),size = smp_siz)  
test <- -train
data_train <- College[train,]
data_test <- College[test,]
```
(b). Fit a linear model using least squares on the training set, and report the test error obtained.
```{r}
lm_model <- lm(Apps ~ ., data = data_train)
lm_predict <- predict(lm_model, data_test)
lm_predict

test_error_least_square = mean((data_test[, 'Apps'] - lm_predict)^2)
test_error_least_square
```

**For the next series of questions, we need our feature data to be in** 
**matrix form because glmnet() requires it. Thus we create a train and test**
**matrix containing only feature values from our respectful train and test data**
```{r}
x_mat_train <- model.matrix(Apps ~ ., data = data_train)[, -1]
x_mat_test <- model.matrix(Apps ~ ., data = data_test)[, -1]
```
(c). Fit a ridge regression model on the training set, with $\lambda$
chosen by cross-validation. Report the test error obtained.
**Extract the optimal lambda for lasso using cv.glmnet()**
```{r}
lambda_min_ridge <- cv.glmnet(x_mat_train, data_train[, 'Apps'], alpha = 0)$lambda.min
lambda_min_ridge
```

**Fit a ridge_model using glmnet() with the optimal lambda**
**Then, call predict() to get the prediction values to ultimately compute the test error**
```{r}
ridge_model <- glmnet(x_mat_train, data_train[, 'Apps'], lambda = lambda_min_ridge, alpha = 0) 
ridge_predict <- predict(ridge_model, newx = x_mat_test, s = lambda_min_ridge)

test_error_ridge = mean((data_test[, 'Apps'] - ridge_predict)^2)
test_error_ridge
```
(d). Fit a lasso model on the training set, with $\lambda$ chosen by cross-validation. Report the test error obtained, along with the number of non-zero coefficient estimates.

**Extract the optimal lambda for lasso using cv.glmnet()**
```{r}
lambda_min_lasso <- cv.glmnet(x_mat_train, data_train[, 'Apps'], alpha = 1)$lambda.min
lambda_min_lasso
```

**Fit a ridge_model using glmnet() with the optimal lambda**
**Then, call predict() to get the prediction values to ultimately compute the test error**
```{r}
lasso_model <- glmnet(x_mat_train, data_train[, 'Apps'], lambda = lambda_min_lasso, alpha = 1) 
lasso_predict <- predict(lasso_model, newx = x_mat_test, s = lambda_min_lasso)

test_error_lasso = mean((data_test[, 'Apps'] - lasso_predict)^2)
test_error_lasso
```

**Extract the coefficients from the lasso model**
```{r}
coef_lasso <- coef(lasso_model)
coef_lasso
```

**Filter out the coefficients equal to zero. Note this process removes the names**
```{r}
coef_lasso_vec <- coef_lasso %>% as.vector()
coef_lasso_vec
coef_lasso_non_zero <- subset(coef_lasso_vec, !(coef_lasso_vec %in% 0.0))
coef_lasso_non_zero
```

(e). Fit a PCR model on the training set, with M chosen by cross validation. Report the test error obtained, along with the value of M selected by cross-validation.

**Fit a PCR model using Cross Validation**
```{r}
set.seed(1)
pcr_model <- pcr(Apps ~., data = data_train, scale = TRUE,
                 validation = "CV")

summary(pcr_model)
```

**Generate a validation plot to determine the optimal value of M for pcr**
**Based on the plot, M is either 5 or 6. We shall go with 6**
```{r}
validationplot(pcr_model, val.type = "MSEP")
```

**Report the test error obtained for pcr**
```{r}
pcr_predict <- predict(pcr_model, x_mat_test, ncomp = 6)
test_error_pcr <- mean((data_test[, 'Apps'] - pcr_predict)^2)
test_error_pcr
```

(f). Fit a PLS model on the training set, with M chosen by cross-validation. Report the test error obtained, along with the value of M selected by cross-validation.

**Fit a PLS model using Cross Validation**
```{r}
set.seed(1)
pls_model <- plsr(Apps ~ ., data = data_train, scale = TRUE, validation = "CV")

summary(pls_model)
```

**Generate a validation plot to determine the optimal value of M for pls**
**Based on the plot, M is either 6 or 7. We shall go with 6**
```{r}
validationplot(pls_model, val.type = "MSEP")
```

**Report the test error obtained for pcr**
```{r}
pls_predict <- predict(pls_model, x_mat_test, ncomp = 6)
test_error_pls <- mean((data_test[, 'Apps'] - pls_predict)^2)
test_error_pls
```
(g). Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these five approaches?

```{r}
lm_test_r2 <- 1 - test_error_least_square / mean((data_test[, 'Apps'] - mean(data_test[, 'Apps']))^2)
ridge_test_r2 <- 1 - test_error_ridge / mean((data_test[, 'Apps'] - mean(data_test[, 'Apps']))^2)
lasso_test_r2 <- 1 - test_error_lasso / mean((data_test[, 'Apps'] - mean(data_test[, 'Apps']))^2)
pcr_test_r2 <- 1 - test_error_pcr / mean((data_test[, 'Apps'] - mean(data_test[, 'Apps']))^2)
pls_test_r2 <- 1 - test_error_pls / mean((data_test[, 'Apps'] - mean(data_test[, 'Apps']))^2)
```
**OLS**
```{r}
cat("Test Error:", test_error_least_square, "\n")
cat("R squared:", lm_test_r2)
```

**Ridge**
```{r}
cat("Test Error:", test_error_ridge, "\n")
cat("R squared:", ridge_test_r2)
```
**Lasso**
```{r}
cat("Test Error:", test_error_lasso, "\n")
cat("R squared:", lasso_test_r2)
```

**PCR**
```{r}
cat("Test Error:", test_error_pcr, "\n")
cat("R squared:", pcr_test_r2)
```


**PLS**
```{r}
cat("Test Error:", test_error_pls, "\n")
cat("R squared:", pls_test_r2)
```


**Discussion:**
**All the models have similar test error.**
**With regards to R squared, all the models produced similar values close to 1.**
**Surprisingly, PCR had significantly lower R_squared then the rest and unsurprisingly,**
**Ridge had the highest**
**All models could be used to accurately predict college applications**

