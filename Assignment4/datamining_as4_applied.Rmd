---
title: "Data Mining HW4 Applied"
author: "Yaniv Bronshtein"
date: "5/7/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Import the necessary libraries**
```{r}
library(ISLR)
library(tidyverse)
library(leaps)
library(gam)
library(MASS)
library(tree)
library(gbm)
library(glmnet)
library(randomForest)

```

#Question 2: GAM Problem 10 Pg 300 of ISL
a). Split the data into a training set and a test set. Using out-of-state tuition
as the response and the other variables as the predictors, perform forward stepwise
selection on the training set in order to identify a satisfactory model that uses 
just a subset of the predictors

```{r}
college_t = College %>% as_tibble()
set.seed(1)
train <- sample(nrow(college_t) * .7)
test <- -train
college_train <- college_t[train, ]
college_test <- college_t[test, ]
```


**Fit the forward stepwise selection model**
```{r}
reg_fit_fwd <- leaps::regsubsets(Outstate ~ ., data = college_train, 
                             nvmax=17, method = "forward")
```

**Extract the metrics from the summary object to perform analysis**
```{r}
reg_summary <- summary(reg_fit_fwd)
cp <- reg_summary$cp
bic <- reg_summary$bic
adjr2 <- reg_summary$adjr2
```

**Create plots to determine the number of predictors**
```{r}
par(mfrow=c(2, 2))
xtick <- seq(1, 17, by = 1)

# Cp plot
plot(cp, xlab="Number of Predictors",ylab="Cp",type='l')
axis(side = 1, at = xtick, labels = FALSE)
min_cp <-  min(cp)
std_cp <-  sd(cp)
abline(h=min_cp + std_cp/sqrt(length(cp)), col="magenta", lty=2)
abline(v=which(cp < min_cp + std_cp/sqrt(length(cp)))[1], col="green", lty=2)


# BIC plot
plot(bic, xlab="Number of Predictors",ylab="BIC",type='l')
axis(side = 1, at = xtick, labels = FALSE)
min_bic <- min(bic)
std_bic <-  sd(bic)
abline(h = min_bic + std_bic, col="magenta", lty=2)
abline(v=which(bic < min_bic + std_bic/sqrt(length(bic)))[1], col="green", lty=2)



# Adjusted R^2 plot
plot(adjr2, xlab="Number of Predictors",
     ylab="Adjusted R2",type='l', ylim=c(0.4, 0.84))
axis(side = 1, at = xtick, labels = FALSE)
max_adjr2 <-  max(reg_summary$adjr2)
std_adjr2 <-  sd(reg_summary$adjr2)
abline(h=max_adjr2 - std_adjr2, col="magenta", lty=2)
abline(v=which(max_adjr2 - std_adjr2/sqrt(length(adjr2)) < adjr2)[1], col="green", lty=2)



```

**Based on the plots, we will take the conservative BIC estimate of 6 predictors**
```{r}
coef(reg_fit_fwd, 6)
```

b). Fit a GAM on the training data, using out-of-state tuition as the response
```{r}
gam_model <- gam(Outstate ~ Private + s(Room.Board, df=2) + s(PhD, df=2) + 
                   s(perc.alumni, df=2) + s(Expend, df=2) + s(Grad.Rate, df=2), 
                 data=college_train)

par(mfrow=c(2,3))
plot(gam_model, se=TRUE, col="blue")
```

*Based on the plots, holding all other variables constant, Private increases the cost*
*of Out of state tuition. The other variables Room.Board, PhD, perc.alumni, Expend,*
*and Grad.Rate show the same trend*

c). Evaluate the model obtained on the test set, and explain the results obtained

**Determine the gam RMSE**
```{r}
gam_pred <- predict(gam_model, college_test)
gam_rmse <- sqrt(mean((college_test$Outstate - gam_pred)^2))
gam_rmse
```
**Determine the gam r^2**
```{r}
gam_r2 <- 1 - (sum((college_test$Outstate - gam_pred)^2) /
                 sum((college_test$Outstate - mean(college_test$Outstate))^2)) 
gam_r2
```
*We get an RMSE of 1984.385 and an R^2 of 0.7614328 or around 76%. This is a very*
*strong result*
**Let us print the summary object for the GAM model**
```{r}
summary(gam_model)
```
*Based on the plots and summary, holding all other variables constant, *
*Private increases the cost of Out of state tuition. The other variables*
*Room.Board, PhD, perc.alumni, Expend, and Grad.Rate show the same trend*

d). For which variables, if any, is there evidence of a non-linear relationship
with the response?
*Based on the summary,  we can see a non-linear relationship between*
*out-of-state tuition and instructional expenditure per student and a non-linear*
*relationship between out-of-state tuition and percent of faculty with PhD's.*
*To a lesser degree, we see a non-linear relationship between out-of-state* 
*tuition and room and board costs*

# Question 3 Decision Tree. Proble 9 at Page 334 of ISL

a). Create a training set containing a random sample of 800 observations,
and a test set containing the remaining observations
```{r}
data(OJ)
set.seed(1)
train <- sample(800)
test <- -train
oj_train <- OJ[train, ]
oj_test <- OJ[test, ]
```


b).Fit a tree to the training data , with Purchase as the response and the other 
variables except for Buy as predictors. Use the summary() function to produce
summary statistics about the tree, and describe the results obtained. What is
the training error rate? How many terminal nodes does the tree have?

```{r}
tree_model <- tree(Purchase ~., data = oj_train)
summary(tree_model)
```

*The tree obtained has 7 terminal nodes with an error rate of 0.165 or 16.5%*
*Two variables were used in the tree construction: LoyalCH and PriceDiff,*
*suggesting that these were the only features influencing customer purchases*

c).Type in the name of the tree object in order to get a detailed text output.
Pick one of the terminal nodes, and interpret the information displayed.
```{r}
tree_model
```
*Suppose a customer scored LoyalCH >=0.51. They will be predicted to be of class*
*CH. Thus, we expect them to purchase Citrus Hill instead of Minute Maid*


d). Create a plot of the tree, and interpret the results.
```{r}
par(mfrow=c(1,1))
plot(tree_model)
text(tree_model, pretty = 0)
```
*The plot gives the same results as the printed model. Given information about*
*our customer, we can use the visualization to predict which orange juice brand*
*they will purchase*

e).Predict the response on the test data, and produce a confusion matrix
comparing the test labels to the predicted test labels. What is the test error
rate?

```{r}
tree_pred <- predict(tree_model, oj_test, type = "class")
table(tree_pred, oj_test$Purchase)
```
**Determine the test error for the tree prediction**
```{r}
test_error <- mean(tree_pred != oj_test$Purchase)
test_error

```
*The test error is 0.2185185 or around 21.85%*

f).Apply the cv.tree() function to the training set in order to determine the
optimal tree size.

```{r}
cv_tree <- cv.tree(tree_model, FUN = prune.misclass)
cv_tree
```
g).Produce a plot with tree size on the x-axis and cross-validated classification
error rate on the y-axis
```{r}
plot(cv_tree$size, cv_tree$dev, type = "b", xlab = "Tree Size", ylab = "CV Error")
```
h). Which tree size corresponds to the lowest cross-validated classification 
error rate on the y-axis?
*The tree of size four has the lowest classification error rate*

i).Produce a pruned tree corresponding to the optimal tree size obtained using
cross validation. If cross-validation does not lead to selection of a pruned tree,
then create a pruned tree
```{r}
pruned_tree <- prune.misclass(tree_model, best = 4)
plot(pruned_tree)
text(pruned_tree, pretty = 0)
```
j). Compare the training error rates between the pruned and unpruned trees.
Which is higher?
```{r}
summary(pruned_tree)
```
*According to the summary, the pruned tree error rate is 16.5%*
*This is the same error rate as for the unpruned tree*

k).Compare the test error rates between the pruned and unpruned trees. 
Which is higher?
```{r}
pruned_tree_pred = predict(pruned_tree,oj_test, type = "class")
pruned_test_error <- mean(oj_test$Purchase != pruned_tree_pred)
pruned_test_error
table(tree_pred, oj_test$Purchase)

```
*As we see from the pruned_test_error value and confusion matrix, the two test errors*
*are the same*

#Problem 4 Bagging and Boosting. Problem 10 at page 334-335 of ISL.
10. We now use boosting to predict Salary in the Hitters data set.

a). Remove the observations from whom the salary information is unknown,
and then log-transform the salaries
```{r}
data("Hitters")

full_dataset <- Hitters[!is.na(Hitters$Salary), ]
full_dataset$Salary <- log10(full_dataset$Salary)

```

b).Create a training set consisting of the first 200 observations, and a test
set consisting of the remaining observations
```{r}
train <- 1:200
test <- -train
hitters_train <- full_dataset[train,]
hitters_test <- full_dataset[test,]
```

c).Performing boosting on the training set with 1000 trees for a range of
values of the shrinkage parameter lambda. Produce a plot with different shrinkage
values on the x-axis and the corresponding training set MSE on the y-axis.

**Perform boosting**
```{r}
pows <-  seq(-9, -0.3, by=0.1)
shrinkage <-  10 ^ pows
len_shrinkage <- length(shrinkage) 
train_mse <- rep(NA, len_shrinkage)
test_mse <- rep(NA, len_shrinkage)
for (i in seq(len_shrinkage)) {
  set.seed(1)
  boost <- gbm(Salary ~., data = hitters_train, distribution = "gaussian",
               n.trees = 1000, shrinkage = shrinkage[i], verbose = FALSE)
  boost_pred_train <- predict(boost, hitters_train, n.trees = 1000)
  boost_pred_test <- predict(boost, hitters_test, n.trees = 1000)
  train_mse[i] <- mean((boost_pred_train - hitters_train$Salary)^2)
  test_mse[i] <- mean((boost_pred_test - hitters_test$Salary)^2)
  
}
```
**Create the plot**
```{r}
plot(shrinkage, train_mse, type="b", 
     xlab="Shrinkage", ylab="Train MSE", 
     col="blue", pch=20)
```
d).Produce a plot with different shrinkage values on the x-axis
and the corresponding test set MSE on the y-axis
```{r}
plot(shrinkage, test_mse, type="b", 
     xlab="Shrinkage", ylab="Test MSE", 
     col="green", pch=20)
```
e).Compare the test MSE of boosting to the test MSE that results
from applying two of the regression approaches seen in Chapters 3 and 6

**Calculate the boosting minimal test mse**
```{r}
min_test_mse <- min(test_mse)
min_test_mse
```
*The minimum test mse for boosting is 0.04662511*

**Determine the best shrinkage lambda from the vector of lambdas to train the final gbm**
```{r}
best_shrinkage <- shrinkage[which.min(test_mse)]
best_shrinkage
```
*Min test error obtained for lambda=0.1*

**Now let us train a linear model**
```{r}
lm_model <- lm(Salary ~. , data = hitters_train)
lm_pred <- predict(lm_model, hitters_test)
mean_lm_test_mse <- mean((hitters_test$Salary - lm_pred)^2)
mean_lm_test_mse
```
*The minimum test mse for linear regression is 0.09275847*

**Let's create the matrices necessary to fit ridge and lasso models**
```{r}
x <- model.matrix(Salary~., hitters_train)
x_test <- model.matrix(Salary ~ . , hitters_test)
y <- hitters_train$Salary

```

**Now let us train a Ridge Model**
```{r}
set.seed(1)
cv_ridge <- cv.glmnet(x = x, y = y, alpha = 0)
best_lambda_ridge <- cv_ridge$lambda.min
ridge_model <- glmnet(x, y, alpha = 0, lambda = best_lambda_ridge)
ridge_pred <- predict(ridge_model, s = best_lambda_ridge, x_test)
ridge_mse <- mean((ridge_pred - hitters_test$Salary)^2)
ridge_mse
```
*The test mse for ridge regression is 0.08617622*

**Now let us train a Lasso Model**
```{r}
set.seed(1)
cv_lasso <- cv.glmnet(x = x, y = y, alpha = 1)
best_lambda_lasso <- cv_lasso$lambda.min
lasso_model <- glmnet(x, y, alpha = 1, lambda = best_lambda_lasso)
lasso_pred <- predict(lasso_model, s = best_lambda_lasso, x_test)
lasso_mse <- mean((lasso_pred - hitters_test$Salary)^2)
lasso_mse

```
*The lasso MSE is 0.08882712*
*In conclusion, the test MSE obtained using gradient boosting is around half of*
*the best two regression methods from the previous chapters (lasso and ridge.)*
*Surprisingly, lasso performs worse than ridge. The Linear Model is predictably last*

f). Which variables appear to be the most important predictors in the boosted
model?

```{r}
set.seed(1)
best_boost <- gbm(Salary ~., data = hitters_train, distribution = "gaussian",
                  n.trees = 1000, shrinkage=best_shrinkage)
summary(best_boost)

```
*Based on the summary, CAtBat is by far the most important variable.*

g).Now apply bagging to the training set. What is the test set MSE for this
approach?
```{r}
set.seed(1)
bag_model <- randomForest(Salary ~., 
                          data = hitters_train, mtry = 19, importance=TRUE)

bag_predict <- predict(bag_model, hitters_test)
bag_test_error <- mean((bag_predict - hitters_test$Salary)^2)
bag_test_error

```
*The bag test error is 0.04342996*

# Problem 5 Hierarchical Clustering. Problem 9 at page 416-417 of ISL

a).Using hierarchical clustering with complete linkage and Euclidean distance,
cluster the states
```{r}
set.seed(1)
hc_complete = hclust(dist(USArrests), method="complete")
plot(hc_complete)
```


b).Cut the dendrogram at a height that results in three distinct clusters.
Which states belong to which clusters?
```{r}
table(cutree(hc_complete, 3))
cutree(hc_complete, 3)
```

c).Hierarchically cluster the states using complete linkage and Euclidean
distance, after scaling the variables to have standard deviation one.
```{r}
hc_scale <- hclust(dist(scale(USArrests)), method = "complete")
plot(hc_scale)

```
*d).What effect does scaling the variables have on the hierarchical clustering* 
*obtained? In your opinion, should the variables be scaled before the inter-observation*
*dissimilarities are computed? Provide a justification for your answer.*
*Scaling the data causes the size of the third clster to increase a lot to include*
*around one third of all the data. In my opinion, variables in clustering algorithms*
*should always be pre-scaled. This is because if each variable has a different scale,* 
*certain data points would gravitate towards a cluster.*





